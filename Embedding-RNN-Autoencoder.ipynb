{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:55.793290Z",
     "start_time": "2017-05-07T22:42:55.655769Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn, ones\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "from torch.nn.init import kaiming_normal\n",
    "from torch import np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with embeddings - simple classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T15:25:27.962041Z",
     "start_time": "2017-05-07T15:25:27.959472Z"
    }
   },
   "source": [
    "## Encoding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:55.799264Z",
     "start_time": "2017-05-07T22:42:55.794673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<BEGIN>': 0, '<STOP>': 1, 'clear': 2, 'cloudy': 3, 'haze': 4, 'partly_cloudy': 5, 'agriculture': 6, 'artisinal_mine': 7, 'bare_ground': 8, 'blooming': 9, 'blow_down': 10, 'conventional_mine': 11, 'cultivation': 12, 'habitation': 13, 'primary': 14, 'road': 15, 'selective_logging': 16, 'slash_burn': 17, 'water': 18}\n"
     ]
    }
   ],
   "source": [
    "vocab = ['<BEGIN>','<STOP>','clear', 'cloudy', 'haze','partly_cloudy',\n",
    "    'agriculture','artisinal_mine','bare_ground','blooming',\n",
    "    'blow_down','conventional_mine','cultivation','habitation',\n",
    "    'primary','road','selective_logging','slash_burn','water'\n",
    "    ]\n",
    "\n",
    "word_to_ix = { word: i for i, word in enumerate(vocab) }\n",
    "print(word_to_ix)\n",
    "one_hot_mapping = {k:np.eye(19)[v] for k,v in word_to_ix.items()}\n",
    "# print(one_hot_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:55.805921Z",
     "start_time": "2017-05-07T22:42:55.800228Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def label_to_target(word_to_ix, label):\n",
    "    return Variable(torch.LongTensor(\n",
    "            list(map(lambda w: word_to_ix[w], label))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T14:20:34.444983Z",
     "start_time": "2017-05-07T14:20:34.441966Z"
    }
   },
   "source": [
    "## Decoding functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:55.815437Z",
     "start_time": "2017-05-07T22:42:55.807317Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ix_to_word = {v: k for k, v in word_to_ix.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:55.825116Z",
     "start_time": "2017-05-07T22:42:55.816881Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "def predictions_to_label(ix_to_word, predictions):\n",
    "    predictions = F.softmax(predictions)\n",
    "    _, preds = torch.max(predictions.data, 1)\n",
    "    return list(map(lambda ix: ix_to_word[ix], flatten(preds.tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Batch creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:55.834188Z",
     "start_time": "2017-05-07T22:42:55.826359Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_batch(n, vocab):\n",
    "    batch = []\n",
    "    for _ in range(n):\n",
    "        batch.append(random.choice(vocab))\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:55.846048Z",
     "start_time": "2017-05-07T22:42:55.835207Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingIdentity(nn.Module):\n",
    "    \"\"\" Testing weight sharing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab, repr_dim, num_rnn_layers):\n",
    "        super(EmbeddingIdentity, self).__init__()\n",
    "        self.label_to_ix = { label: i for i, label in enumerate(vocab) }\n",
    "        self.embeds = nn.Embedding(len(vocab), repr_dim)\n",
    "        self.rnn = nn.LSTM(input_size=repr_dim,\n",
    "                            hidden_size=repr_dim,\n",
    "                            num_layers=num_rnn_layers,\n",
    "                            batch_first = True)\n",
    "        self.fc = nn.Linear(repr_dim, len(vocab))\n",
    "        \n",
    "        # link embedding and decoding weight\n",
    "        self.fc.weight = self.embeds.weight\n",
    "    \n",
    "    def toVariable(self, x):\n",
    "        return Variable(torch.LongTensor(\n",
    "            list(map(lambda lbl: self.label_to_ix[lbl], x))\n",
    "        ))\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.toVariable(x)          # Tensor with a single integer\n",
    "        f = self.embeds(x).unsqueeze(1) # Dim 1x5 --> unsqueeze --> 1x1x5\n",
    "        f, hidden = self.rnn(f, hidden) # Dim output: 1x1x5, Dim hidden: 2x1x5\n",
    "        f = self.fc(f.contiguous().squeeze(1)) # Dim 1x19\n",
    "        return f\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:55.856676Z",
     "start_time": "2017-05-07T22:42:55.847035Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = EmbeddingIdentity(vocab,5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:55.885821Z",
     "start_time": "2017-05-07T22:42:55.857716Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blow_down']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model([\"slash_burn\"])\n",
    "predictions_to_label(ix_to_word, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:55.892235Z",
     "start_time": "2017-05-07T22:42:55.889413Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = gen_batch(10, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:55.900117Z",
     "start_time": "2017-05-07T22:42:55.893226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blow_down',\n",
       " 'road',\n",
       " 'road',\n",
       " 'road',\n",
       " 'road',\n",
       " 'road',\n",
       " 'road',\n",
       " 'blow_down',\n",
       " 'road',\n",
       " 'blow_down']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(batch)\n",
    "predictions_to_label(ix_to_word, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:55.904333Z",
     "start_time": "2017-05-07T22:42:55.901486Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conventional_mine',\n",
       " 'clear',\n",
       " 'partly_cloudy',\n",
       " 'bare_ground',\n",
       " 'partly_cloudy',\n",
       " 'clear',\n",
       " 'bare_ground',\n",
       " 'road',\n",
       " 'cultivation',\n",
       " 'blow_down']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = label_to_target(word_to_ix,batch)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:55.910734Z",
     "start_time": "2017-05-07T22:42:55.907992Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = F.cross_entropy(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:55.918650Z",
     "start_time": "2017-05-07T22:42:55.914286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 2.8663\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping up for multi epoch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:55.924403Z",
     "start_time": "2017-05-07T22:42:55.922195Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.464338Z",
     "start_time": "2017-05-07T22:42:55.925374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.956784963607788\n",
      "1 2.948615550994873\n",
      "2 2.9271082878112793\n",
      "3 2.9263296127319336\n",
      "4 2.920184373855591\n",
      "5 2.911189079284668\n",
      "6 2.908919334411621\n",
      "7 2.898547410964966\n",
      "8 2.8742473125457764\n",
      "9 2.8789680004119873\n",
      "10 2.832608222961426\n",
      "11 2.818241596221924\n",
      "12 2.8162808418273926\n",
      "13 2.827798366546631\n",
      "14 2.7939224243164062\n",
      "15 2.749051094055176\n",
      "16 2.751620054244995\n",
      "17 2.716921329498291\n",
      "18 2.683095693588257\n",
      "19 2.679623603820801\n",
      "20 2.6578989028930664\n",
      "21 2.6117377281188965\n",
      "22 2.588899612426758\n",
      "23 2.5576703548431396\n",
      "24 2.558809995651245\n",
      "25 2.5392045974731445\n",
      "26 2.4743964672088623\n",
      "27 2.456502676010132\n",
      "28 2.4482438564300537\n",
      "29 2.4007177352905273\n",
      "30 2.37410044670105\n",
      "31 2.3194262981414795\n",
      "32 2.295959711074829\n",
      "33 2.2708818912506104\n",
      "34 2.2550437450408936\n",
      "35 2.195267915725708\n",
      "36 2.178271770477295\n",
      "37 2.1516690254211426\n",
      "38 2.1237802505493164\n",
      "39 2.0647482872009277\n",
      "40 2.022016763687134\n",
      "41 1.9893934726715088\n",
      "42 1.9478552341461182\n",
      "43 1.9277807474136353\n",
      "44 1.9139608144760132\n",
      "45 1.8647056818008423\n",
      "46 1.8194890022277832\n",
      "47 1.7837674617767334\n",
      "48 1.7647148370742798\n",
      "49 1.7362864017486572\n",
      "50 1.6998568773269653\n",
      "51 1.6637630462646484\n",
      "52 1.5983728170394897\n",
      "53 1.6278507709503174\n",
      "54 1.5557664632797241\n",
      "55 1.540797233581543\n",
      "56 1.5265151262283325\n",
      "57 1.4989259243011475\n",
      "58 1.4830081462860107\n",
      "59 1.4518612623214722\n",
      "60 1.410485863685608\n",
      "61 1.3570468425750732\n",
      "62 1.333226203918457\n",
      "63 1.3303022384643555\n",
      "64 1.3181143999099731\n",
      "65 1.3190244436264038\n",
      "66 1.3006846904754639\n",
      "67 1.259911060333252\n",
      "68 1.2347248792648315\n",
      "69 1.2252098321914673\n",
      "70 1.1896125078201294\n",
      "71 1.1850883960723877\n",
      "72 1.152129054069519\n",
      "73 1.1467409133911133\n",
      "74 1.1512874364852905\n",
      "75 1.1024279594421387\n",
      "76 1.071515679359436\n",
      "77 1.097636103630066\n",
      "78 1.0476388931274414\n",
      "79 1.0289279222488403\n",
      "80 1.0123482942581177\n",
      "81 0.9951173067092896\n",
      "82 0.987876296043396\n",
      "83 0.9868976473808289\n",
      "84 0.9389702677726746\n",
      "85 0.933320164680481\n",
      "86 0.9306677579879761\n",
      "87 0.916660487651825\n",
      "88 0.8981791734695435\n",
      "89 0.8966394662857056\n",
      "90 0.8998153209686279\n",
      "91 0.8781679272651672\n",
      "92 0.8439000844955444\n",
      "93 0.8305489420890808\n",
      "94 0.8173628449440002\n",
      "95 0.833016574382782\n",
      "96 0.7827537059783936\n",
      "97 0.7845514416694641\n",
      "98 0.7807004451751709\n",
      "99 0.7652389407157898\n",
      "100 0.7529506087303162\n",
      "101 0.740496039390564\n",
      "102 0.7273309230804443\n",
      "103 0.7197161316871643\n",
      "104 0.7143945097923279\n",
      "105 0.7067319750785828\n",
      "106 0.690339207649231\n",
      "107 0.6794791221618652\n",
      "108 0.671931803226471\n",
      "109 0.6671802401542664\n",
      "110 0.6480058431625366\n",
      "111 0.6537505984306335\n",
      "112 0.6351231336593628\n",
      "113 0.6251122355461121\n",
      "114 0.6212114095687866\n",
      "115 0.5876771807670593\n",
      "116 0.5862895846366882\n",
      "117 0.5913501977920532\n",
      "118 0.5760325789451599\n",
      "119 0.5671560168266296\n",
      "120 0.5549911856651306\n",
      "121 0.556772768497467\n",
      "122 0.5372502207756042\n",
      "123 0.5314541459083557\n",
      "124 0.5247998237609863\n",
      "125 0.5194250345230103\n",
      "126 0.4963558614253998\n",
      "127 0.4966796040534973\n",
      "128 0.5075283646583557\n",
      "129 0.49669787287712097\n",
      "130 0.4723891615867615\n",
      "131 0.4758055806159973\n",
      "132 0.4764394462108612\n",
      "133 0.4727155864238739\n",
      "134 0.4577498435974121\n",
      "135 0.4449952244758606\n",
      "136 0.45292675495147705\n",
      "137 0.43525615334510803\n",
      "138 0.4442201852798462\n",
      "139 0.42732134461402893\n",
      "140 0.42095476388931274\n",
      "141 0.42358729243278503\n",
      "142 0.40287521481513977\n",
      "143 0.40413838624954224\n",
      "144 0.401177316904068\n",
      "145 0.3933658003807068\n",
      "146 0.385789692401886\n",
      "147 0.3880433142185211\n",
      "148 0.38366788625717163\n",
      "149 0.3710204064846039\n",
      "150 0.36231276392936707\n",
      "151 0.3570611774921417\n",
      "152 0.3618173897266388\n",
      "153 0.3499341905117035\n",
      "154 0.34277230501174927\n",
      "155 0.3456561863422394\n",
      "156 0.33930644392967224\n",
      "157 0.33945927023887634\n",
      "158 0.3370533287525177\n",
      "159 0.3231652081012726\n",
      "160 0.32107821106910706\n",
      "161 0.3181910216808319\n",
      "162 0.3126486539840698\n",
      "163 0.3205187916755676\n",
      "164 0.3046862781047821\n",
      "165 0.3049655854701996\n",
      "166 0.3043077886104584\n",
      "167 0.2954113185405731\n",
      "168 0.2924692630767822\n",
      "169 0.291413277387619\n",
      "170 0.2836947739124298\n",
      "171 0.277910977602005\n",
      "172 0.2801235318183899\n",
      "173 0.27262619137763977\n",
      "174 0.27389466762542725\n",
      "175 0.26891934871673584\n",
      "176 0.2707962989807129\n",
      "177 0.2662341296672821\n",
      "178 0.2587500214576721\n",
      "179 0.26036712527275085\n",
      "180 0.2554924190044403\n",
      "181 0.24954494833946228\n",
      "182 0.24819695949554443\n",
      "183 0.25309255719184875\n",
      "184 0.24575383961200714\n",
      "185 0.24346572160720825\n",
      "186 0.23725461959838867\n",
      "187 0.24236173927783966\n",
      "188 0.23759795725345612\n",
      "189 0.2302592247724533\n",
      "190 0.23529015481472015\n",
      "191 0.22655032575130463\n",
      "192 0.2298617660999298\n",
      "193 0.22172072529792786\n",
      "194 0.22537249326705933\n",
      "195 0.21906107664108276\n",
      "196 0.21597027778625488\n",
      "197 0.20900724828243256\n",
      "198 0.2154749482870102\n",
      "199 0.2082587331533432\n",
      "200 0.21167267858982086\n",
      "201 0.20977936685085297\n",
      "202 0.2036387026309967\n",
      "203 0.20208784937858582\n",
      "204 0.19638396799564362\n",
      "205 0.19380943477153778\n",
      "206 0.19006338715553284\n",
      "207 0.1897169053554535\n",
      "208 0.1915450394153595\n",
      "209 0.19054244458675385\n",
      "210 0.190226212143898\n",
      "211 0.18842460215091705\n",
      "212 0.18778489530086517\n",
      "213 0.181130051612854\n",
      "214 0.18027549982070923\n",
      "215 0.18214276432991028\n",
      "216 0.1763402670621872\n",
      "217 0.17161394655704498\n",
      "218 0.1731691211462021\n",
      "219 0.17231391370296478\n",
      "220 0.1661520153284073\n",
      "221 0.168998122215271\n",
      "222 0.16664691269397736\n",
      "223 0.16288542747497559\n",
      "224 0.1612737774848938\n",
      "225 0.15881597995758057\n",
      "226 0.16014809906482697\n",
      "227 0.16187019646167755\n",
      "228 0.15868321061134338\n",
      "229 0.15385179221630096\n",
      "230 0.1556016206741333\n",
      "231 0.154999241232872\n",
      "232 0.15561532974243164\n",
      "233 0.15262888371944427\n",
      "234 0.15136803686618805\n",
      "235 0.14948581159114838\n",
      "236 0.1472756415605545\n",
      "237 0.14816489815711975\n",
      "238 0.14439575374126434\n",
      "239 0.14628973603248596\n",
      "240 0.1457638293504715\n",
      "241 0.14106222987174988\n",
      "242 0.13797403872013092\n",
      "243 0.14156386256217957\n",
      "244 0.13845747709274292\n",
      "245 0.1364104300737381\n",
      "246 0.1355668306350708\n",
      "247 0.13424400985240936\n",
      "248 0.13252273201942444\n",
      "249 0.13238778710365295\n",
      "250 0.13397030532360077\n",
      "251 0.12961962819099426\n",
      "252 0.12714992463588715\n",
      "253 0.12847116589546204\n",
      "254 0.12902480363845825\n",
      "255 0.12930968403816223\n",
      "256 0.1254555583000183\n",
      "257 0.12490958720445633\n",
      "258 0.12540443241596222\n",
      "259 0.12183753401041031\n",
      "260 0.12237770855426788\n",
      "261 0.11936774849891663\n",
      "262 0.11976402997970581\n",
      "263 0.11671845614910126\n",
      "264 0.1202169731259346\n",
      "265 0.11903568357229233\n",
      "266 0.11376915872097015\n",
      "267 0.11614760011434555\n",
      "268 0.1158352643251419\n",
      "269 0.11369992792606354\n",
      "270 0.1118394061923027\n",
      "271 0.11094670742750168\n",
      "272 0.11156199872493744\n",
      "273 0.11208020895719528\n",
      "274 0.10812782496213913\n",
      "275 0.10962977260351181\n",
      "276 0.10863091051578522\n",
      "277 0.10681947320699692\n",
      "278 0.1053599938750267\n",
      "279 0.1043616533279419\n",
      "280 0.10474538058042526\n",
      "281 0.10379241406917572\n",
      "282 0.10336485505104065\n",
      "283 0.10345002263784409\n",
      "284 0.1033203974366188\n",
      "285 0.10097571462392807\n",
      "286 0.09929781407117844\n",
      "287 0.10263426601886749\n",
      "288 0.09960710257291794\n",
      "289 0.09822684526443481\n",
      "290 0.09947282075881958\n",
      "291 0.09979031980037689\n",
      "292 0.09802750498056412\n",
      "293 0.09671236574649811\n",
      "294 0.09503449499607086\n",
      "295 0.09528698027133942\n",
      "296 0.09576146304607391\n",
      "297 0.09509167075157166\n",
      "298 0.09293882548809052\n",
      "299 0.09210740774869919\n",
      "300 0.09219960123300552\n",
      "301 0.09139654040336609\n",
      "302 0.09146631509065628\n",
      "303 0.09047780185937881\n",
      "304 0.08911822736263275\n",
      "305 0.08917694538831711\n",
      "306 0.08990646153688431\n",
      "307 0.08854206651449203\n",
      "308 0.08983474969863892\n",
      "309 0.088066965341568\n",
      "310 0.08613695949316025\n",
      "311 0.08581741154193878\n",
      "312 0.08567482233047485\n",
      "313 0.08401264995336533\n",
      "314 0.08344060182571411\n",
      "315 0.084859699010849\n",
      "316 0.08494531363248825\n",
      "317 0.08337301760911942\n",
      "318 0.08159329742193222\n",
      "319 0.08190447092056274\n",
      "320 0.08138462901115417\n",
      "321 0.08565954864025116\n",
      "322 0.07944423705339432\n",
      "323 0.08110705763101578\n",
      "324 0.08123282343149185\n",
      "325 0.08044125884771347\n",
      "326 0.0807039737701416\n",
      "327 0.07777375727891922\n",
      "328 0.07753735035657883\n",
      "329 0.07819315791130066\n",
      "330 0.07805044203996658\n",
      "331 0.07634882628917694\n",
      "332 0.0760447308421135\n",
      "333 0.07406728714704514\n",
      "334 0.07470335066318512\n",
      "335 0.0753820613026619\n",
      "336 0.07313065230846405\n",
      "337 0.07440075278282166\n",
      "338 0.07515144348144531\n",
      "339 0.07313158363103867\n",
      "340 0.07325034588575363\n",
      "341 0.07304782420396805\n",
      "342 0.07274697721004486\n",
      "343 0.07177308946847916\n",
      "344 0.07009008526802063\n",
      "345 0.07074572145938873\n",
      "346 0.07112521678209305\n",
      "347 0.07009164988994598\n",
      "348 0.07104455679655075\n",
      "349 0.06920284032821655\n",
      "350 0.06894547492265701\n",
      "351 0.06832563132047653\n",
      "352 0.0692233294248581\n",
      "353 0.06908603012561798\n",
      "354 0.06737317889928818\n",
      "355 0.06669493019580841\n",
      "356 0.06826364248991013\n",
      "357 0.06766297668218613\n",
      "358 0.06698358058929443\n",
      "359 0.06584599614143372\n",
      "360 0.06611011177301407\n",
      "361 0.06554559618234634\n",
      "362 0.06444717943668365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363 0.06316755712032318\n",
      "364 0.06349195539951324\n",
      "365 0.06464570015668869\n",
      "366 0.06357749551534653\n",
      "367 0.062733955681324\n",
      "368 0.06362716853618622\n",
      "369 0.06219123676419258\n",
      "370 0.06315423548221588\n",
      "371 0.062006816267967224\n",
      "372 0.061410821974277496\n",
      "373 0.06166284903883934\n",
      "374 0.06015673279762268\n",
      "375 0.06161753460764885\n",
      "376 0.06003640592098236\n",
      "377 0.061790332198143005\n",
      "378 0.06077992916107178\n",
      "379 0.059719160199165344\n",
      "380 0.060314472764730453\n",
      "381 0.05991772189736366\n",
      "382 0.0600455217063427\n",
      "383 0.059739623218774796\n",
      "384 0.060548458248376846\n",
      "385 0.05752124264836311\n",
      "386 0.0569709874689579\n",
      "387 0.05664600431919098\n",
      "388 0.056815046817064285\n",
      "389 0.05699184909462929\n",
      "390 0.05662521347403526\n",
      "391 0.056607685983181\n",
      "392 0.05597366392612457\n",
      "393 0.057010751217603683\n",
      "394 0.055294740945100784\n",
      "395 0.056630298495292664\n",
      "396 0.054974328726530075\n",
      "397 0.05528252571821213\n",
      "398 0.05522580444812775\n",
      "399 0.05457230284810066\n",
      "400 0.05341123789548874\n",
      "401 0.05369950458407402\n",
      "402 0.053978804498910904\n",
      "403 0.053411927074193954\n",
      "404 0.05174202471971512\n",
      "405 0.053131021559238434\n",
      "406 0.05230196937918663\n",
      "407 0.051099419593811035\n",
      "408 0.052268847823143005\n",
      "409 0.05230187252163887\n",
      "410 0.0531432181596756\n",
      "411 0.051191043108701706\n",
      "412 0.050297532230615616\n",
      "413 0.05100643262267113\n",
      "414 0.05087539553642273\n",
      "415 0.05206119641661644\n",
      "416 0.049583930522203445\n",
      "417 0.04958900809288025\n",
      "418 0.050176385790109634\n",
      "419 0.05006033182144165\n",
      "420 0.04930032789707184\n",
      "421 0.0499315969645977\n",
      "422 0.05032942071557045\n",
      "423 0.049683328717947006\n",
      "424 0.04784092307090759\n",
      "425 0.05037360265851021\n",
      "426 0.048110201954841614\n",
      "427 0.04846329614520073\n",
      "428 0.048471011221408844\n",
      "429 0.04684774577617645\n",
      "430 0.04766704887151718\n",
      "431 0.04814049229025841\n",
      "432 0.04619891196489334\n",
      "433 0.046687427908182144\n",
      "434 0.04718215391039848\n",
      "435 0.04678726941347122\n",
      "436 0.04621358588337898\n",
      "437 0.04622626677155495\n",
      "438 0.045962050557136536\n",
      "439 0.04555550217628479\n",
      "440 0.04477955028414726\n",
      "441 0.04476730898022652\n",
      "442 0.04404669255018234\n",
      "443 0.04407403618097305\n",
      "444 0.044944681227207184\n",
      "445 0.044954702258110046\n",
      "446 0.04473065957427025\n",
      "447 0.04528024420142174\n",
      "448 0.0450621023774147\n",
      "449 0.04323938488960266\n",
      "450 0.043604202568531036\n",
      "451 0.04392962157726288\n",
      "452 0.04430774226784706\n",
      "453 0.04425658658146858\n",
      "454 0.044146377593278885\n",
      "455 0.04301688075065613\n",
      "456 0.0427689328789711\n",
      "457 0.042483314871788025\n",
      "458 0.044347889721393585\n",
      "459 0.042221978306770325\n",
      "460 0.041684750467538834\n",
      "461 0.042212147265672684\n",
      "462 0.04119304195046425\n",
      "463 0.04230358451604843\n",
      "464 0.04118760675191879\n",
      "465 0.041583698242902756\n",
      "466 0.04158180579543114\n",
      "467 0.04245099425315857\n",
      "468 0.04096466302871704\n",
      "469 0.04052310436964035\n",
      "470 0.04119236022233963\n",
      "471 0.04033290594816208\n",
      "472 0.04058797284960747\n",
      "473 0.03941723704338074\n",
      "474 0.03995921090245247\n",
      "475 0.039474934339523315\n",
      "476 0.03933800011873245\n",
      "477 0.03989081829786301\n",
      "478 0.03974369913339615\n",
      "479 0.038595449179410934\n",
      "480 0.038587916642427444\n",
      "481 0.03871512785553932\n",
      "482 0.03864739090204239\n",
      "483 0.03830672428011894\n",
      "484 0.038950588554143906\n",
      "485 0.03904297575354576\n",
      "486 0.03820271044969559\n",
      "487 0.03771372511982918\n",
      "488 0.037960730493068695\n",
      "489 0.03787723556160927\n",
      "490 0.038747768849134445\n",
      "491 0.03712095320224762\n",
      "492 0.03774121776223183\n",
      "493 0.03776343911886215\n",
      "494 0.03707615286111832\n",
      "495 0.03737765923142433\n",
      "496 0.03625714033842087\n",
      "497 0.03768520429730415\n",
      "498 0.03699130192399025\n",
      "499 0.03658464178442955\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    X = gen_batch(500, vocab)\n",
    "    y = label_to_target(word_to_ix,X)\n",
    "    y_pred = model(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(epoch, loss.data[0])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.472183Z",
     "start_time": "2017-05-07T22:42:58.468916Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch2 = gen_batch(30, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.480332Z",
     "start_time": "2017-05-07T22:42:58.476559Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output2 = model(batch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.490163Z",
     "start_time": "2017-05-07T22:42:58.484756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<STOP>',\n",
       " 'haze',\n",
       " 'conventional_mine',\n",
       " 'conventional_mine',\n",
       " 'bare_ground',\n",
       " 'artisinal_mine',\n",
       " 'bare_ground',\n",
       " 'habitation',\n",
       " 'cultivation',\n",
       " 'blow_down',\n",
       " 'selective_logging',\n",
       " '<STOP>',\n",
       " 'blooming',\n",
       " 'primary',\n",
       " 'road',\n",
       " 'blow_down',\n",
       " 'blooming',\n",
       " '<STOP>',\n",
       " 'partly_cloudy',\n",
       " '<BEGIN>',\n",
       " 'blooming',\n",
       " 'blow_down',\n",
       " 'cloudy',\n",
       " 'bare_ground',\n",
       " 'primary',\n",
       " 'primary',\n",
       " 'cultivation',\n",
       " 'agriculture',\n",
       " 'partly_cloudy',\n",
       " 'artisinal_mine']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.500240Z",
     "start_time": "2017-05-07T22:42:58.494514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<STOP>',\n",
       " 'haze',\n",
       " 'conventional_mine',\n",
       " 'conventional_mine',\n",
       " 'bare_ground',\n",
       " 'artisinal_mine',\n",
       " 'bare_ground',\n",
       " 'habitation',\n",
       " 'cultivation',\n",
       " 'blow_down',\n",
       " 'selective_logging',\n",
       " '<STOP>',\n",
       " 'blooming',\n",
       " 'primary',\n",
       " 'road',\n",
       " 'blow_down',\n",
       " 'blooming',\n",
       " '<STOP>',\n",
       " 'partly_cloudy',\n",
       " '<BEGIN>',\n",
       " 'blooming',\n",
       " 'blow_down',\n",
       " 'cloudy',\n",
       " 'bare_ground',\n",
       " 'primary',\n",
       " 'primary',\n",
       " 'cultivation',\n",
       " 'agriculture',\n",
       " 'partly_cloudy',\n",
       " 'artisinal_mine']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_to_label(ix_to_word, output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.507632Z",
     "start_time": "2017-05-07T22:42:58.504587Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.515764Z",
     "start_time": "2017-05-07T22:42:58.511970Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_vocab = ['clear', 'cloudy', 'haze','partly_cloudy',\n",
    "    'agriculture','artisinal_mine','bare_ground','blooming',\n",
    "    'blow_down','conventional_mine','cultivation','habitation',\n",
    "    'primary','road','selective_logging','slash_burn','water'\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.526126Z",
     "start_time": "2017-05-07T22:42:58.520113Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_batch_sequences(n, seq_vocab):\n",
    "    batch = []\n",
    "    for _ in range(n):\n",
    "        seq = ['<BEGIN>']\n",
    "        for _ in range(random.randint(1,16)):\n",
    "            seq.append(random.choice(seq_vocab))\n",
    "        seq.append('<STOP>')\n",
    "        seq = list(dict.fromkeys(seq)) # Remove duplicate while keeping order\n",
    "        batch.append(seq)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.533751Z",
     "start_time": "2017-05-07T22:42:58.530624Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_seq = gen_batch_sequences(10, seq_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.542640Z",
     "start_time": "2017-05-07T22:42:58.538078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<BEGIN>', 'clear', 'artisinal_mine', 'road', '<STOP>'],\n",
       " ['<BEGIN>',\n",
       "  'primary',\n",
       "  'blooming',\n",
       "  'cloudy',\n",
       "  'blow_down',\n",
       "  'cultivation',\n",
       "  'slash_burn',\n",
       "  'road',\n",
       "  '<STOP>'],\n",
       " ['<BEGIN>', 'blow_down', 'water', '<STOP>'],\n",
       " ['<BEGIN>',\n",
       "  'primary',\n",
       "  'blooming',\n",
       "  'haze',\n",
       "  'blow_down',\n",
       "  'road',\n",
       "  'cloudy',\n",
       "  'habitation',\n",
       "  'conventional_mine',\n",
       "  '<STOP>'],\n",
       " ['<BEGIN>',\n",
       "  'conventional_mine',\n",
       "  'road',\n",
       "  'water',\n",
       "  'cultivation',\n",
       "  'agriculture',\n",
       "  'haze',\n",
       "  'primary',\n",
       "  'slash_burn',\n",
       "  '<STOP>'],\n",
       " ['<BEGIN>', 'haze', 'road', '<STOP>'],\n",
       " ['<BEGIN>',\n",
       "  'partly_cloudy',\n",
       "  'cloudy',\n",
       "  'slash_burn',\n",
       "  'blooming',\n",
       "  'road',\n",
       "  'cultivation',\n",
       "  'artisinal_mine',\n",
       "  'clear',\n",
       "  'blow_down',\n",
       "  'water',\n",
       "  '<STOP>'],\n",
       " ['<BEGIN>',\n",
       "  'artisinal_mine',\n",
       "  'blow_down',\n",
       "  'conventional_mine',\n",
       "  'cloudy',\n",
       "  'slash_burn',\n",
       "  'water',\n",
       "  '<STOP>'],\n",
       " ['<BEGIN>',\n",
       "  'partly_cloudy',\n",
       "  'road',\n",
       "  'selective_logging',\n",
       "  'primary',\n",
       "  'habitation',\n",
       "  'clear',\n",
       "  '<STOP>'],\n",
       " ['<BEGIN>', 'cultivation', 'primary', 'water', '<STOP>']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.546935Z",
     "start_time": "2017-05-07T22:42:58.543720Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seqlabels_to_target(word_to_ix, list_seq):\n",
    "    return list(map(lambda s: torch.LongTensor(\n",
    "                list(map(lambda label: word_to_ix[label], s))\n",
    "            ), list_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.551728Z",
     "start_time": "2017-05-07T22:42:58.547974Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeds = nn.Embedding(len(vocab), 5, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.561171Z",
     "start_time": "2017-05-07T22:42:58.552724Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_tensors = seqlabels_to_target(word_to_ix, batch_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.566755Z",
     "start_time": "2017-05-07T22:42:58.562174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "  0\n",
       " 14\n",
       "  9\n",
       "  3\n",
       " 10\n",
       " 12\n",
       " 17\n",
       " 15\n",
       "  1\n",
       "[torch.LongTensor of size 9]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Variable(seq_tensors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.572439Z",
     "start_time": "2017-05-07T22:42:58.567745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "-0.5254 -0.1480  0.9374 -1.9755 -0.1513\n",
       "-0.5471 -1.0386  0.7887 -0.1657 -0.9696\n",
       " 1.4302  1.0649  2.6553  1.7606  0.7829\n",
       " 1.0784  1.1442  0.7852 -0.6828 -0.9015\n",
       " 0.1928  0.2651 -0.3466 -0.9127  0.5378\n",
       " 0.1592 -0.4425  0.0053  1.4737 -0.8462\n",
       "-1.5880  0.7203 -0.0336  0.7798 -0.4408\n",
       "-0.0589  1.1673  1.1468 -0.3766  0.6624\n",
       "[torch.FloatTensor of size 9x5]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds(Variable(seq_tensors[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.582710Z",
     "start_time": "2017-05-07T22:42:58.573624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "  0\n",
       " 14\n",
       "  9\n",
       "  4\n",
       " 10\n",
       " 15\n",
       "  3\n",
       " 13\n",
       " 11\n",
       "  1\n",
       "[torch.LongTensor of size 10]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Variable(seq_tensors[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.588219Z",
     "start_time": "2017-05-07T22:42:58.583690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0000  0.0000  0.0000  0.0000  0.0000\n",
       "-0.5254 -0.1480  0.9374 -1.9755 -0.1513\n",
       "-0.5471 -1.0386  0.7887 -0.1657 -0.9696\n",
       " 2.2629 -0.9940  0.0123 -0.3606  0.8853\n",
       " 1.0784  1.1442  0.7852 -0.6828 -0.9015\n",
       "-1.5880  0.7203 -0.0336  0.7798 -0.4408\n",
       " 1.4302  1.0649  2.6553  1.7606  0.7829\n",
       "-1.4367 -0.9439 -0.1380 -1.5372  1.1991\n",
       " 0.7725  1.8602 -0.2360 -0.3035  0.3017\n",
       "-0.0589  1.1673  1.1468 -0.3766  0.6624\n",
       "[torch.FloatTensor of size 10x5]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds(Variable(seq_tensors[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like it will be a pain to work with variable size input. It would need my custom data loader. Hence I would create a custom one directly for Amazon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.751839Z",
     "start_time": "2017-05-07T22:42:58.589356Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from torch import np, from_numpy # Numpy like wrapper\n",
    "\n",
    "class TagsDataset(Dataset):\n",
    "    \"\"\"Dataset wrapping target labels for Kaggle - Planet Amazon from Space competition.\n",
    "\n",
    "    Arguments:\n",
    "        A CSV file path\n",
    "        Path to image folder\n",
    "        Extension of images\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, vocab_mapping):\n",
    "    \n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.vocab_mapping = vocab_mapping\n",
    "\n",
    "        self.tags = self.df['tags'].str.split()\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        vocab = self.vocab_mapping\n",
    "        tags = []\n",
    "        tags.append(vocab['<BEGIN>'])\n",
    "        tags.extend([vocab[tag] for tag in self.tags[index]])\n",
    "        tags.append(vocab['<STOP>'])\n",
    "        \n",
    "        tags = torch.Tensor(tags)\n",
    "        \n",
    "        return tags, tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index)\n",
    "\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        \"\"\"Creates mini-batch tensors for tags with variable size\n",
    "\n",
    "        Args:\n",
    "            data: list of tuple (input, target). \n",
    "                - input: torch tensor of shape (?); variable length.\n",
    "                - target: torch tensor of same shape (?); variable length.\n",
    "        Returns:\n",
    "            inputs: torch tensor of shape (batch_size, padded_length).\n",
    "            targets: torch tensor of shape (batch_size, padded_length).\n",
    "            lengths: list; valid length for each padded tags.\n",
    "        \"\"\"\n",
    "        # Sort a data list by target length (descending order).\n",
    "        data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "        _, tags = zip(*data)\n",
    "\n",
    "        # Merge tags (from tuple of 1D tensor to 2D tensor).\n",
    "        lengths = [len(tag) for tag in tags]\n",
    "        targets = torch.zeros(len(tags), max(lengths)).long()\n",
    "        for i, tag in enumerate(tags):\n",
    "            end = lengths[i]\n",
    "            targets[i, :end] = tag[:end]        \n",
    "        return targets, targets, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.813294Z",
     "start_time": "2017-05-07T22:42:58.753040Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = TagsDataset('./data/train.csv',word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:42:58.816903Z",
     "start_time": "2017-05-07T22:42:58.814528Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=X_train, \n",
    "                                              batch_size=100,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=1,\n",
    "                                              collate_fn=X_train.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-08T06:39:02.728583Z",
     "start_time": "2017-05-08T06:39:02.713415Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SeqPred(nn.Module):\n",
    "    \"\"\" Testing weight sharing + Variable Length sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, num_rnn_layers):\n",
    "        super(SeqPred, self).__init__()\n",
    " \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeds = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.LSTM(embed_dim, embed_dim, num_rnn_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "        # link embedding and decoding weight\n",
    "        self.fc.weight = self.embeds.weight\n",
    "    \n",
    "    \n",
    "    def forward(self, tags, lengths, hidden=None):\n",
    "        embed = self.embeds(tags)\n",
    "        packed = pack_padded_sequence(embed, lengths, batch_first=True)\n",
    "        out, hidden = self.rnn(packed, hidden)\n",
    "        out = self.fc(out[0])\n",
    "        return out, hidden\n",
    "\n",
    "    def genTags(self, inputs, states):\n",
    "        tag_ids = []\n",
    "        inputs = self.embeds(inputs)\n",
    "        for i in range(self.vocab_size):                                      # maximum sampling length\n",
    "            hiddens, states = self.rnn(inputs, states)          # (batch_size, 1, hidden_size)\n",
    "            outputs = self.fc(hiddens.squeeze(1))            # (batch_size, vocab_size)\n",
    "            predicted = outputs.max(1)[1]\n",
    "            tag_ids.append(predicted)\n",
    "            inputs = self.embeds(predicted)\n",
    "        tag_ids = torch.cat(tag_ids, 1)                  # (batch_size, 19)\n",
    "        return tag_ids.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-08T06:39:03.828023Z",
     "start_time": "2017-05-08T06:39:03.825008Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SeqPred(19, 5, 2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-08T06:39:05.208978Z",
     "start_time": "2017-05-08T06:39:05.179013Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected a Variable argument, but got tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-ad0cc646fcbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-61cd10ee82b1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tags, lengths, hidden)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mout_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mout_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected a Variable argument, but got tuple"
     ]
    }
   ],
   "source": [
    "epoch =0\n",
    "for batch_idx, (data, target, lengths) in enumerate(train_loader):\n",
    "    data = Variable(data)\n",
    "    target = Variable(target)\n",
    "    targets = pack_padded_sequence(target, lengths, batch_first=True)[0]\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    outputs, _ = model(data,lengths)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch_idx % 100 == 0:\n",
    "        print('Train Epoch: {:03d} [{:05d}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader) * len(data),\n",
    "            100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:49:14.444418Z",
     "start_time": "2017-05-07T22:49:14.442008Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set initial states\n",
    "state = (Variable(torch.zeros(2, 1, 5)),\n",
    "             Variable(torch.zeros(2, 1, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:55:22.587314Z",
     "start_time": "2017-05-07T22:55:22.583825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 12\n",
       "[torch.LongTensor of size 1x1]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = Variable(torch.rand(1, 1).mul(19).long(), volatile=True)\n",
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:55:25.327619Z",
     "start_time": "2017-05-07T22:55:25.324650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       "[torch.LongTensor of size 1x1]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start0 = Variable(torch.zeros(1, 1).long(), volatile=True)\n",
    "start0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-07T22:55:40.310568Z",
     "start_time": "2017-05-07T22:55:40.300867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       " 0\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 6\n",
       "[torch.LongTensor of size 19]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.genTags(start0,state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
